# Module 12: Using Machine Learning with HDInsight

## Lab: Using machine learning with HDInsight

### Exercise 1: Provision an HDInsight cluster

#### Task 1: Provision an HDInsight cluster

1. Ensure that the **MT17B-WS2016-NAT**, **20774A-LON-DC**, and **20774A-LON-DEV** virtual machines are running, and that you are logged on to **20774A-LON-DEV** as **ADATUM\\AdatumAdmin**.
2. On the Start menu, start to type **Internet Explorer**, and then click **Internet Explorer**.
3. In the address bar, type **http://azure.microsoft.com**, click **Portal**, and then sign in using the Microsoft account that is associated with your Azure Learning Pass subscription.
4. In the Azure Portal, click **All resources**, and then verify that there are no existing HDInsight clusters in your subscription.
5. In the Azure Portal, in the left pane, click **+ Create a resource**.
6. Click **Analytics**, and then click **HDInsight**.
7. On the **Basics** blade, type the following details, and then click **Cluster type**:
    - **Cluster name**: &lt;your name&gt;&lt;date&gt;
8. Enter the following details, and then click **Select**:
    - **Cluster type**: Hadoop
    - **Operating system**: Linux
    - **Version**: (Leave at default)
9. Enter the following details, and then click **Next**:
    - **Cluster login username**: hadmin
    - **Cluster login password**: Pa55w.rdPa55w.rd
    - **Secure Shell (SSH) username**: sadmin
    - **Resource group (Create new)**: hadooprg
    - **Location**: Select your region
10. On the **Storage** blade, under **Select a Storage account**, click **Create new**.
11. In the **Create a new Storage account** box, type &lt;***your name***&gt;&lt;***date***&gt;.
12. In the **Default container** box, replace the suggested name with the name of your cluster (for example, &lt;***your name***&gt;&lt;***date***&gt;.
13. Leave all other settings at their defaults, and then click **Next**.
14. On the **Cluster summary** blade, next to **Cluster size**, click **Edit**.
15. On the **Cluster size** blade, in the **Number of Worker nodes** box, type **1**.
16. Click **Worker node size**.
17. On the **Choose your node size** blade, click **View all**, click **A3 General Purpose**, and then click **Select**.
18. Click **Head node size**.
19. On the **Choose your node size** blade, click **View all**, click **A3 General Purpose**, and then click **Select**.
20. On the **Cluster size** blade, click **Next**.
21. On the **Advanced settings** blade, click **Next**.
22. On the **Cluster summary** blade, note the estimated cost per hour of this cluster; to avoid using up your Azure Pass allowance, it is important that you remove the cluster when you are not using it.
23. On the **Cluster summary** blade, click **Create**.
24. The deployment might take 20–30 minutes to complete. Wait for the cluster to be provisioned and do not continue with this exercise until it has.

#### Task 2: View the cluster configuration in the Azure Portal

1. In the Azure Portal, click **All resources**, and then click your cluster.
2. On the **HDInsight Cluster** blade, ensure the **Status** shows as **Running**, and view the summary information for your cluster.
3. On the **HDInsight Cluster** blade, under **Settings**, click **Cluster size**, and note that you can dynamically scale the number of worker nodes to meet processing demand.
4. On the **HDInsight Cluster** blade, click **Overview**, and then click **Cluster dashboards**.
5. On the **Cluster dashboards** blade, click **Ambari home**.
6. In the **Windows Security** dialog box, log in as **hadmin** with a password of **Pa55w.rdPa55w.rd**.
7. Note the various metrics provided by the Ambari web application.
8. Close the Ambari tab.

#### Task 3: Connect to the HDInsight cluster using SSH

1. In the Azure Portal, close the **Cluster dashboards** blade.
2. Under **Settings**, click **SSH + Cluster login**.
3. In the **Hostname list**, click your cluster, and then copy the host name **&lt;*your name*&gt;&lt;*date*&gt;-ssh.azurehdinsight.net** to the clipboard.
4. Open a Windows command prompt.
5. At the command prompt , type **putty**, and then press Enter.
6. In the **PuTTY Configuration** window, on the **Session** page, paste the host name from step 2 into the **Host Name** box.
7. Under **Connection type**, select **SSH**, and then click **Open**.
8. If a security warning that the host certificate cannot be verified is displayed, click **Yes** to continue.
9. When you are prompted, enter **sadmin** and **Pa55w.rdPa55w.rd** as the credentials.

#### Task 4: Browse cluster storage

> **Note**: The commands in this task are case-sensitive. They can be copied from **E:\\Labfiles\\Lab12\\Ex1Task4Cmds.txt** and pasted by right-clicking the SSH console window.

1. In the SSH console window, type the following command to view the contents of the root folder in the HDFS file system:
    ```
    hdfs dfs -ls /
    ```
2. Type the following command to view the contents of the **/example** folder in the HDFS file system. This folder contains subfolders for sample apps, data, and JAR components:
    ```
    hdfs dfs -ls /example
    ```
3. Type the following command to view the contents of the **/example/data/gutenberg** folder, which contains sample text files:
    ```
    hdfs dfs -ls /example/data/gutenberg
    ```
4. Type the following command on one line to view the text in the **ulysses.txt** file:
    ```
    hdfs dfs -text /example/data/gutenberg/ulysses.txt
    ```
5. Note that the file contains large volumes of unstructured text.

**Results:** At the end of this exercise, you will have provisioned an HDInsight cluster.

### Exercise 2: Use HDInsight for MapReduce and Spark jobs

#### Task 1: Run the MapReduce word count job

> **Note**: The commands in this task are case-sensitive. They can be copied from **E:\\Labfiles\\Lab12\\Ex2Task1Cmds.txt** and pasted by right-clicking the SSH console window.

1. In the SSH console window, type the following command, and then press Enter:
    ```
    ls /usr/hdp/current/hadoop-mapreduce-client
    ```
2. Note the sample JAR files stored in the cluster head node.
3. Type the following command, and then press Enter:
    ```
    hadoop jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar
    ```
4. Note the list of MapReduce functions in the **hadoop-mapreduce-examples.jar** file.
5. Type the following command, and then press Enter:
    ```
    hadoop jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar wordcount
    ```
6. Note the help text for the **wordcount** function.
7. Type the following command, and then press Enter:
    ```
    hadoop jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar wordcount /example/data/gutenberg/davinci.txt /example/results
    ```
8. Note that this command runs a MapReduce job to process a text file (**davinci.txt**) and store the results in the **/example/results** folder.
9. When the MapReduce job has completed, type the following command, and then press Enter:
    ```
    hdfs dfs -ls /example/results
    ```
10. Note that a file named **part-r-00000** has been created in the **results** folder.
11. Type the following command, and then press Enter:
    ```
    hdfs dfs -text /example/results/part-r-00000
    ```
12. Note the type of data in the output file.
13. Minimize the SSH console window.

#### Task 2: Download and view output from the MapReduce job

1. On the Start menu, type **Azure Storage**, and then click **Microsoft Azure Storage Explorer**.
2. In the **Connect to Azure Storage** dialog box, ensure that **Add an** **Azure Account** is selected, and then click **Sign in**.
3. In the **Sign in to your account** dialog box, enter the credentials of the Microsoft account that is associated with your Azure Learning Pass subscription, and then click **Sign in**.
4. Under your Azure Learning Pass subscription, under **Storage Accounts**, expand the storage account as created in the previous tasks **&lt;*your name*&gt;&lt;*date*&gt;**.
5. Expand **Blob Containers**, and then click the Blob container as created in the previous tasks **&lt;*your name*&gt;&lt;*date*&gt;**.
6. In the right pane, browse to the **example/results** folder.
7. Double-click the **part-r-00000** text file to download it.
8. In the **Microsoft Azure Storage Explorer** dialog box, click **Yes**.
9. When you are prompted, double-click **Notepad** to open it.
10. View the word counts for the review data.
11. In Notepad, on the **File** menu, click **Save As**, and then save the file as **MapReduceResults.tsv** in the **Documents** folder.
12. Close Notepad.
13. Click **Start**, type **Excel**, and then click **Excel 2016**.
14. If the **First things first** dialog box appears, click **Ask me later**, and then click **Accept**.
15. In Excel, click **Open Other Workbooks**.
16. On the **Open** page, click **This PC**, and then click **Documents**.
17. In the **Open** dialog box, in the **File Type** list, click **All Files (\*.\*)**, click **MapReduceResults.tsv**, and then click **Open**.
18. In the **Text Import Wizard - Step 1 of 3** page, ensure that **Delimited** is selected, and then click **Next**.
19. In the **Text Import Wizard - Step 2 of 3** page, ensure that **Tab** is selected, and then click **Next**.
20. In the **Text Import Wizard - Step 3 of 3** page, click **Finish**.
21. Click column **A** to select all of the data in that column.
22. On the status bar, in the lower-right corner, **Count** shows the number of rows; make a note of this number.
23. Click **Sort & Filter**, and then click **Custom Sort**.
24. In the **Sort Warning** dialog box, click **Expand the selection**, and then click **Sort**.
25. In the **Sort** dialog box, under **Column**, select **Column B**.
26. Under **Order**, select **Largest to Smallest**, and then click **OK**.
27. Note that the commonest words are **the**, **of**, and **and**.
28. Close the **MapReduceResults.tsv** file, without saving any changes.

#### Task 3: Use the Python shell to submit commands to Spark

> **Note**: The commands in this task are case-sensitive. They can be copied from **E:\\Labfiles\\Lab12\\Ex2Task3Cmds.txt**, and pasted by right-clicking the SSH console window.

1. In the SSH console window, type the following command, and then press Enter:
    ```
    pyspark
    ```
2. Note that the Python Spark shell may take a few minutes to start.
3. Type the following command, and then press Enter:
    ```
    alltxt = sc.textFile("/example/data/gutenberg/davinci.txt")
    ```
4. Type the following command, and then press Enter:
    ```
    alltxt.count()
    ```
5. Note the number of lines of text reported in the text file.
6. Type the following command, and then press Enter:
    ```
    alltxt.first()
    ```
7. Note the first line shown from the text file.
8. Type the following command, and then press Enter:
    ```
    filtertxt = alltxt.filter(lambda alltxt: "Italy" in alltxt)
    ```
9. Note that this command creates a new RDD named **filtertxt** that filters the text so that only lines containing the word **Italy** are included.
10. Type the following command, and then press Enter:
      ```
      filtertxt.count()
      ```
11. Note that the number of rows in the **filtertxt** RDD is shown.
12. Type the following command, and then press Enter:
      ```
      filtertxt.collect()
      ```
13. Note that this command displays the contents of the **filtertxt** RDD.
14. Type the following command, and then press Enter:
      ```
      quit()
      ```
15. Press Enter again to return to the command line.

#### Task 4: Use Python script to submit an application to Spark

> **Note**: The commands in this task are case-sensitive. They can be copied from **E:\\Labfiles\\Lab12\\Ex2Task4Cmds.txt** and pasted by right-clicking the SSH console window.

1. On the taskbar, click **File** **Explorer**, and then browse to **E:\\Labfiles\\Lab12**.
2. Double-click **SparkScript.txt** to open the file in Notepad.
3. Select the contents of the file, and then copy them to the clipboard.
4. Switch to the SSH console window, type the following command, and then press Enter:
   ```
   nano SparkCount.py
   ```
5. Right-click the console window to paste the script that you copied previously.
6. Verify that the editor contains the following code:
    ```
    # import and initialize Spark context
    from pyspark import SparkConf, SparkContext
    cnf = SparkConf().setMaster("local").setAppName("SparkCount")
    sc = SparkContext(conf = cnf)
    # use Spark to count words
    alltxt = sc.textFile("/example/data/gutenberg/outlineofscience.txt")
    words = alltxt.flatMap(lambda alltxt: alltxt.split(" "))
    counts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a +b)
    # store results
    counts.saveAsTextFile("/example/spark_output")
    ```
7. Press CTRL+X, and then press Y and Enter to save SparkCount.py.
8. Type the following command, and then press Enter:
   ```
    spark-submit SparkCount.py
   ```
9.  Note that the SparkCount.py Python script has been submitted to Spark.
10. Wait for the script to complete, type the following command, and then press Enter:
    ```
    hdfs dfs -ls /example/spark_output
    ```
11. Note the list of output files.
12. Type the following command, and then press Enter:
    ```
    hdfs dfs -text /example/spark_output/part-00000
    ```
13. Note that the **part-00000** file contains word counts.
14. Minimize the SSH console window.

#### Task 5: Download and view output from Spark job

1. Switch to Azure Storage Explorer.
2. Ensure that, under **Blob Containers**, the Blob container that was created in the preparation steps (**&lt;*your name*&gt;&lt;*date*&gt;**) is still selected.
3. In the right pane, browse to the **example** folder, and then double-click the **spark\_output** folder.
4. Double-click the **part-00000** text file to download it.
5. In the **Microsoft Azure Storage Explorer** dialog box, click **Yes**.
6. When you are prompted, double-click **Notepad** to open it.
7. In Notepad, on the **File** menu, click **Save As**, and then save the file as **SparkResults.csv** in the **Documents** folder.
8. Close Notepad.
9. Start Excel 2016, and then click **Open Other Workbooks**.
10. On the **Open** page, click **This PC**, and then click **Documents**.
11. In the **Open** dialog box, in the **File Type** list, select **All Files (\*.*)**, click **SparkResults.csv**, and then click **Open**.
12. Browse through the first few rows of the data. Note that some further processing would be needed, for example, to remove brackets and control characters.
13. Close Microsoft Excel without saving your changes.
14. Close Azure Storage Explorer, the SSH console session, and Command Prompt windows.
15. In Internet Explorer, click **Resource groups**, right-click **hadooprg**, and then click **Delete resource group**.
16. In the **TYPE THE RESOURCE GROUP NAME** box, type **hadooprg**, and then click **Delete**.

**Results:** At the end of this exercise, you will have run a MapReduce job and used the Python shell to run Spark commands.

©2017 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
